{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "authorship_tag": "ABX9TyP1MqIRqsT+5Eb6gdwcdrLr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vatsal-ts/BTP_MQTT/blob/main/mqtt_prelims.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install scikit-learn\n",
        "!pip install pandas\n",
        "!pip install keras\n",
        "!pip install tensorflow\n",
        "!pip install xgboost\n",
        "!pip install scikeras"
      ],
      "metadata": {
        "id": "QkT_EOCoyQz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBC2MaJKxWkV"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import tree\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from matplotlib import pyplot as pyplot\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "# from sklearn.metrics import plot_confusion_matrix\n",
        "from warnings import simplefilter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwpkpzW7zRp7",
        "outputId": "893662ba-db44-4728-8a04-a53651e7c076"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/mqttset.zip -d /content/mqttset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkbX87iI8JNK",
        "outputId": "e5854194-ad06-46d3-9ce4-598a26826a79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/mqttset.zip\n",
            "  inflating: /content/mqttset/Data/CSV/bruteforce.csv  \n",
            "  inflating: /content/mqttset/Data/CSV/flood.csv  \n",
            "  inflating: /content/mqttset/Data/CSV/legitimate_1w.csv  \n",
            "  inflating: /content/mqttset/Data/CSV/malaria.csv  \n",
            "  inflating: /content/mqttset/Data/CSV/malformed.csv  \n",
            "  inflating: /content/mqttset/Data/CSV/slowite.csv  \n",
            "  inflating: /content/mqttset/Data/FINAL_CSV/mqttdataset_reduced.csv  \n",
            "  inflating: /content/mqttset/Data/FINAL_CSV/test30.csv  \n",
            "  inflating: /content/mqttset/Data/FINAL_CSV/test30_augmented.csv  \n",
            "  inflating: /content/mqttset/Data/FINAL_CSV/test30_reduced.csv  \n",
            "  inflating: /content/mqttset/Data/FINAL_CSV/train70.csv  \n",
            "  inflating: /content/mqttset/Data/FINAL_CSV/train70_augmented.csv  \n",
            "  inflating: /content/mqttset/Data/FINAL_CSV/train70_reduced.csv  \n",
            "  inflating: /content/mqttset/Data/PCAP/bruteforce.pcapng  \n",
            "  inflating: /content/mqttset/Data/PCAP/capture_1w.pcap  \n",
            "  inflating: /content/mqttset/Data/PCAP/capture_flood.pcap  \n",
            "  inflating: /content/mqttset/Data/PCAP/capture_malariaDoS.pcap  \n",
            "  inflating: /content/mqttset/Data/PCAP/malformed.pcap  \n",
            "  inflating: /content/mqttset/Data/PCAP/slowite.pcap  \n",
            "  inflating: /content/mqttset/Data/README.md  \n",
            "  inflating: /content/mqttset/code/parser/rawparsing_augmented.py  \n",
            "  inflating: /content/mqttset/code/parser/rawparsing_normal.py  \n",
            "  inflating: /content/mqttset/code/processingdata_ml.py  \n",
            "  inflating: /content/mqttset/requirements.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start = time.time()\n",
        "\n",
        "#import two dataset splitted\n",
        "dftrain = pd.read_csv(\"/content/mqttset/Data/FINAL_CSV/train70_reduced.csv\")\n",
        "dftest = pd.read_csv(\"/content/mqttset/Data/FINAL_CSV/test30_reduced.csv\")\n",
        "# Transpose the describe result to make columns as rows\n",
        "describe_train = dftrain.describe().transpose()\n",
        "describe_test = dftest.describe().transpose()\n",
        "\n",
        "# Filter columns to drop in dftrain\n",
        "columns_to_drop_train = describe_train[(describe_train['min'] == 0) & (describe_train['max'] == 0)].index.tolist()\n",
        "\n",
        "# Filter columns to drop in dftest\n",
        "columns_to_drop_test = describe_test[(describe_test['min'] == 0) & (describe_test['max'] == 0)].index.tolist()\n",
        "\n",
        "# Drop columns from dftrain and dftest\n",
        "dftrain = dftrain.drop(columns=columns_to_drop_train)\n",
        "dftest = dftest.drop(columns=columns_to_drop_test)\n",
        "simplefilter(action='ignore', category=FutureWarning)\n",
        "seed = 7\n",
        "\n",
        "#train\n",
        "#print(dftrain.loc[dftrain['target'] == 'legitimate'])\n",
        "class_names = dftrain.target.unique()\n",
        "dftrain=dftrain.astype('category')\n",
        "cat_columns = dftrain.select_dtypes(['category']).columns\n",
        "dftrain[cat_columns] = dftrain[cat_columns].apply(lambda x: x.cat.codes)\n",
        "#print(dftrain.loc[125, 'target'])\n",
        "x_columns = dftrain.columns.drop('target')\n",
        "x_train = dftrain[x_columns].values\n",
        "y_train = dftrain['target']\n",
        "x_train=x_train[:-1]\n",
        "y_train=y_train[:-1]\n",
        "\n",
        "\n",
        "#test\n",
        "class_names = dftest.target.unique()\n",
        "dftest=dftest.astype('category')\n",
        "cat_columns = dftest.select_dtypes(['category']).columns\n",
        "dftest[cat_columns] = dftest[cat_columns].apply(lambda x: x.cat.codes)\n",
        "x_columns = dftest.columns.drop('target')\n",
        "x_test = dftest[x_columns].values\n",
        "y_test = dftest['target']\n",
        "\n",
        "\n",
        "print(\"Ready to generate train and test datasets\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMWRkB67xbxs",
        "outputId": "bd08844d-13a4-4c94-a89f-0408b618f17e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ready to generate train and test datasets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Neural network\n",
        "print(\"Starting Random forest\")\n",
        "model = Sequential()\n",
        "model.add(Dense(50, input_dim=x_train.shape[1], kernel_initializer='normal', activation='relu'))\n",
        "model.add(Dense(30, input_dim=x_train.shape[1], kernel_initializer='normal', activation='relu'))\n",
        "model.add(Dense(20, kernel_initializer='normal'))\n",
        "model.add(Dense(6,activation='softmax'))\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
        "history = model.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor],verbose=2,epochs=200,batch_size=1000)\n",
        "end = time.time()\n",
        "diff=end-start\n",
        "print(\"Training time: \" + str(diff))\n",
        "starttest = time.time()\n",
        "y_pred_nn = model.predict(x_test)\n",
        "y_pred_nn = np.argmax(y_pred_nn,axis=1)\n",
        "endtest =time.time()\n",
        "difftest = endtest-starttest\n",
        "print(\"Test time: \" + str(difftest))\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "\n",
        "#Create Naive Bayes Classifier\n",
        "print(\"Starting Naive Bayes\")\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(x_train, y_train)\n",
        "end = time.time()\n",
        "diff=end-start\n",
        "print(\"Training time: \" + str(diff))\n",
        "starttest = time.time()\n",
        "y_pred_nb = gnb.predict(x_test)\n",
        "endtest =time.time()\n",
        "difftest = endtest-starttest\n",
        "print(\"Test time: \" + str(difftest))\n",
        "\n",
        "#Decision tree\n",
        "print(\"Starting Decision tree\")\n",
        "clf = DecisionTreeClassifier()\n",
        "clf = clf.fit(x_train,y_train)\n",
        "end = time.time()\n",
        "diff=end-start\n",
        "print(\"Training time: \" + str(diff))\n",
        "starttest = time.time()\n",
        "y_pred_dt = clf.predict(x_test)\n",
        "y_pred_dt_roc = clf.predict_proba(x_test)\n",
        "endtest =time.time()\n",
        "difftest = endtest-starttest\n",
        "print(\"Test time: \" + str(difftest))\n",
        "\n",
        "\n",
        "print(\"Decision Tree, accuracy: \" + str(metrics.accuracy_score(y_test, y_pred_dt)) + \" F1 score:\" + str(metrics.f1_score(y_test, y_pred_dt,average='weighted')))\n",
        "matrixdt = confusion_matrix(y_test,y_pred_dt)\n",
        "print(matrixdt)\n",
        "\n",
        "\n",
        "print(\"Naive Bayes, accuracy: \" + str(metrics.accuracy_score(y_test, y_pred_nb)) + \" F1 score:\" + str(metrics.f1_score(y_test, y_pred_nb,average='weighted')))\n",
        "matrixnv = confusion_matrix(y_test,y_pred_nb)\n",
        "print(matrixnv)\n",
        "\n",
        "\n",
        "print(\"Neural network, accuracy: \" + str(metrics.accuracy_score(y_test, y_pred_nn)) + \" F1 score:\" + str(metrics.f1_score(y_test, y_pred_nn,average='weighted')))\n",
        "matrixnn = confusion_matrix(y_test,y_pred_nn)\n",
        "print(matrixnn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3wfjQoJyZdW",
        "outputId": "cebb3181-3ea3-41fa-e6e6-d670f84785bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Random forest\n",
            "Epoch 1/200\n",
            "232/232 - 3s - loss: 1.1654 - accuracy: 0.7599 - val_loss: 0.5814 - val_accuracy: 0.7804 - 3s/epoch - 14ms/step\n",
            "Epoch 2/200\n",
            "232/232 - 2s - loss: 0.4979 - accuracy: 0.8068 - val_loss: 0.4769 - val_accuracy: 0.8155 - 2s/epoch - 7ms/step\n",
            "Epoch 3/200\n",
            "232/232 - 3s - loss: 0.3994 - accuracy: 0.8700 - val_loss: 0.3984 - val_accuracy: 0.8625 - 3s/epoch - 11ms/step\n",
            "Epoch 4/200\n",
            "232/232 - 2s - loss: 0.3571 - accuracy: 0.8835 - val_loss: 0.3900 - val_accuracy: 0.8624 - 2s/epoch - 8ms/step\n",
            "Epoch 5/200\n",
            "232/232 - 2s - loss: 0.3175 - accuracy: 0.8910 - val_loss: 0.3775 - val_accuracy: 0.8613 - 2s/epoch - 9ms/step\n",
            "Epoch 6/200\n",
            "232/232 - 2s - loss: 0.2676 - accuracy: 0.9064 - val_loss: 0.4437 - val_accuracy: 0.8424 - 2s/epoch - 8ms/step\n",
            "Epoch 7/200\n",
            "232/232 - 1s - loss: 0.2469 - accuracy: 0.9152 - val_loss: 0.4471 - val_accuracy: 0.8565 - 980ms/epoch - 4ms/step\n",
            "Epoch 8/200\n",
            "232/232 - 1s - loss: 0.2312 - accuracy: 0.9176 - val_loss: 0.5833 - val_accuracy: 0.8456 - 965ms/epoch - 4ms/step\n",
            "Epoch 9/200\n",
            "232/232 - 1s - loss: 0.2234 - accuracy: 0.9205 - val_loss: 0.5002 - val_accuracy: 0.8778 - 977ms/epoch - 4ms/step\n",
            "Epoch 10/200\n",
            "232/232 - 1s - loss: 0.2115 - accuracy: 0.9234 - val_loss: 0.6169 - val_accuracy: 0.8524 - 975ms/epoch - 4ms/step\n",
            "Epoch 10: early stopping\n",
            "Training time: 31.027748584747314\n",
            "3103/3103 [==============================] - 6s 2ms/step\n",
            "Test time: 11.189393758773804\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_8 (Dense)             (None, 50)                1100      \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 30)                1530      \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 20)                620       \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 6)                 126       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3376 (13.19 KB)\n",
            "Trainable params: 3376 (13.19 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Starting Naive Bayes\n",
            "Training time: 42.37487006187439\n",
            "Test time: 0.054373979568481445\n",
            "Starting Decision tree\n",
            "Training time: 42.72889804840088\n",
            "Test time: 0.02033853530883789\n",
            "Decision Tree, accuracy: 0.9031322388961628 F1 score:0.9009088402539802\n",
            "[[ 3313   571     0     7   460     0]\n",
            " [  212 35556     0  3250    59     0]\n",
            " [    1     4    90    88     1     0]\n",
            " [   19  3144     0 46468     8     0]\n",
            " [  992   331    13   456  1486     0]\n",
            " [    0     0     0     0     2  2759]]\n",
            "Naive Bayes, accuracy: 0.670863128210293 F1 score:0.7581608917548786\n",
            "[[ 4290    38     8     5    10     0]\n",
            " [11198 27866     0    13     0     0]\n",
            " [   93     0    89     2     0     0]\n",
            " [18473     0     0 31166     0     0]\n",
            " [ 2676    94    36    34   438     0]\n",
            " [    0     0     0     0     0  2761]]\n",
            "Neural network, accuracy: 0.8523919830798671 F1 score:0.8521993221772527\n",
            "[[ 3746   464     2    22   117     0]\n",
            " [  295 34931     0  3788    63     0]\n",
            " [    1     6    87    88     2     0]\n",
            " [    0  3180     0 42477   278  3704]\n",
            " [ 1676   623     8   314   652     5]\n",
            " [    0     0     0     5    15  2741]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "# Create and train an SVM classifier\n",
        "print(\"Starting Support Vector Machine (SVM)\")\n",
        "svm_classifier = SVC(kernel='poly')  # You can choose different kernels (e.g., 'linear', 'rbf', 'poly', etc.)\n",
        "svm_classifier.fit(x_train, y_train)\n",
        "\n",
        "# Predict using the SVM classifier\n",
        "y_pred_svm = svm_classifier.predict(x_test)\n",
        "\n",
        "# Evaluate the SVM model\n",
        "print(\"Support Vector Machine (SVM), accuracy:\", metrics.accuracy_score(y_test, y_pred_svm))\n",
        "print(\"F1 score:\", metrics.f1_score(y_test, y_pred_svm, average='weighted'))\n",
        "matrix_svm = confusion_matrix(y_test, y_pred_svm)\n",
        "print(matrix_svm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJxwi-vN7BWL",
        "outputId": "086730d0-1d72-4334-da75-bbda44f9b970"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Support Vector Machine (SVM)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Create and train a k-NN classifier\n",
        "print(\"Starting k-Nearest Neighbors (k-NN)\")\n",
        "knn_classifier = KNeighborsClassifier(n_neighbors=5)  # You can adjust the number of neighbors (k) as needed\n",
        "knn_classifier.fit(x_train, y_train)\n",
        "\n",
        "# Predict using the k-NN classifier\n",
        "y_pred_knn = knn_classifier.predict(x_test)\n",
        "\n",
        "# Evaluate the k-NN model\n",
        "print(\"k-Nearest Neighbors (k-NN), accuracy:\", metrics.accuracy_score(y_test, y_pred_knn))\n",
        "print(\"F1 score:\", metrics.f1_score(y_test, y_pred_knn, average='weighted'))\n",
        "matrix_knn = confusion_matrix(y_test, y_pred_knn)\n",
        "print(matrix_knn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "fhkPL_XC6mBP",
        "outputId": "697b1b03-848d-42c9-ed23-aa0eaa8e0e43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting k-Nearest Neighbors (k-NN)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-e8f3b37627a4>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Predict using the k-NN classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0my_pred_knn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mknn_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Evaluate the k-NN model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_classification.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;31m# In that case, we do not need the distances to perform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;31m# the weighting so we do not compute them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             \u001b[0mneigh_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_distance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m             \u001b[0mneigh_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    859\u001b[0m                 \u001b[0mkwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meffective_metric_params_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 861\u001b[0;31m             chunked_results = list(\n\u001b[0m\u001b[1;32m    862\u001b[0m                 pairwise_distances_chunked(\n\u001b[1;32m    863\u001b[0m                     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mpairwise_distances_chunked\u001b[0;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[0m\n\u001b[1;32m   1874\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreduce_func\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1875\u001b[0m             \u001b[0mchunk_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD_chunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1876\u001b[0;31m             \u001b[0mD_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1877\u001b[0m             \u001b[0m_check_chunk_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1878\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mD_chunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py\u001b[0m in \u001b[0;36m_kneighbors_reduce_func\u001b[0;34m(self, dist, start, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    717\u001b[0m         \"\"\"\n\u001b[1;32m    718\u001b[0m         \u001b[0msample_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m         \u001b[0mneigh_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_neighbors\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m         \u001b[0mneigh_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneigh_ind\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;31m# argpartition doesn't guarantee sorted order, so we sort again\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36margpartition\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36margpartition\u001b[0;34m(a, kth, axis, kind, order)\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m     \"\"\"\n\u001b[0;32m--> 845\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argpartition'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Create and train a logistic regression classifier\n",
        "print(\"Starting Logistic Regression\")\n",
        "logistic_classifier = LogisticRegression(solver='liblinear')  # You can choose different solvers based on your data\n",
        "logistic_classifier.fit(x_train, y_train)\n",
        "\n",
        "# Predict using the logistic regression classifier\n",
        "y_pred_logistic = logistic_classifier.predict(x_test)\n",
        "\n",
        "# Evaluate the logistic regression model\n",
        "print(\"Logistic Regression, accuracy:\", metrics.accuracy_score(y_test, y_pred_logistic))\n",
        "print(\"F1 score:\", metrics.f1_score(y_test, y_pred_logistic, average='weighted'))\n",
        "matrix_logistic = confusion_matrix(y_test, y_pred_logistic)\n",
        "print(matrix_logistic)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dePtPhR6pxj",
        "outputId": "69b98a30-5c0e-4fc5-96af-af3536f11328"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Logistic Regression\n",
            "Logistic Regression, accuracy: 0.7939772383925874\n",
            "F1 score: 0.7630931146718104\n",
            "[[  423   425     0  3502     1     0]\n",
            " [    0 27851     0 11226     0     0]\n",
            " [    1     2    87    94     0     0]\n",
            " [    0     0     0 49639     0     0]\n",
            " [    1   283     0  2935    33    26]\n",
            " [    0   115     0  1845     0   801]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier  # If you're using XGBoost for classification\n",
        "from xgboost import XGBRegressor   # If you're using XGBoost for regression\n",
        "\n",
        "# Create and train additional classifiers\n",
        "xgb_classifier= XGBClassifier()\n",
        "rf_classifier = RandomForestClassifier()\n",
        "lr_classifier = LogisticRegression()\n",
        "\n",
        "xgb_classifier.fit(x_train, y_train)\n",
        "rf_classifier.fit(x_train, y_train)\n",
        "lr_classifier.fit(x_train, y_train)\n",
        "\n",
        "# Suppose you have already trained your XGBoost classifier (xgb_classifier)\n",
        "\n",
        "# Create a VotingClassifier with majority voting\n",
        "ensemble_model = VotingClassifier(estimators=[\n",
        "    ('XGBoost', xgb_classifier),\n",
        "    ('Random Forest', rf_classifier),\n",
        "    ('Logistic Regression', lr_classifier)\n",
        "], voting='hard')\n",
        "\n",
        "# Fit the ensemble model on the individual models' predictions\n",
        "ensemble_model.fit(np.array([xgb_classifier.predict(x_test), rf_classifier.predict(x_test), lr_classifier.predict(x_test)]).T, y_test)\n",
        "\n",
        "# Make predictions with the ensemble model\n",
        "y_pred_ensemble = ensemble_model.predict(np.array([xgb_classifier.predict(x_test), rf_classifier.predict(x_test), lr_classifier.predict(x_test)]).T)\n",
        "\n",
        "# Evaluate the ensemble model\n",
        "print(\"Ensemble model, accuracy:\", metrics.accuracy_score(y_test, y_pred_ensemble))\n",
        "print(\"F1 score:\", metrics.f1_score(y_test, y_pred_ensemble, average='weighted'))\n",
        "matrix_ensemble = confusion_matrix(y_test, y_pred_ensemble)\n",
        "print(matrix_ensemble)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3MpI1m76rRL",
        "outputId": "7c1f67b6-a752-4e29-e437-35582b132a3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble model, accuracy: 0.9047033940980965\n",
            "F1 score: 0.9023862131065937\n",
            "[[ 3521   484     0     7   339     0]\n",
            " [  232 35555     0  3250    40     0]\n",
            " [    2     4    89    88     1     0]\n",
            " [   49  3122     0 46468     0     0]\n",
            " [ 1102   276     7   459  1434     0]\n",
            " [    0     0     0     0     0  2761]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Assuming you have loaded and preprocessed your data into x_train, y_train, x_test, y_test\n",
        "\n",
        "# Create a Sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Add a 1D convolutional layer with 32 filters and kernel size 3\n",
        "model.add(Conv1D(32, kernel_size=3, input_shape=(x_train.shape[1], 1), activation='relu'))\n",
        "\n",
        "# Add a 1D MaxPooling layer with pool size 2\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "# Flatten the output of the convolutional layer\n",
        "model.add(Flatten())\n",
        "\n",
        "# Add a fully connected layer with 128 units and ReLU activation\n",
        "model.add(Dense(128, activation='relu'))\n",
        "\n",
        "# Add the output layer with the number of classes and softmax activation\n",
        "model.add(Dense(6, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Define an early stopping callback\n",
        "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train, validation_data=(x_test, y_test), callbacks=[monitor], verbose=2, epochs=200, batch_size=1000)\n",
        "\n",
        "# Measure training time\n",
        "end = time.time()\n",
        "diff = end - start\n",
        "print(\"Training time: \" + str(diff))\n",
        "\n",
        "# Make predictions on the test data\n",
        "starttest = time.time()\n",
        "y_pred_cnn = model.predict(x_test)\n",
        "y_pred_cnn = np.argmax(y_pred_cnn, axis=1)\n",
        "endtest = time.time()\n",
        "difftest = endtest - starttest\n",
        "print(\"Test time: \" + str(difftest))\n",
        "\n",
        "# Print model summary\n",
        "print(model.summary())\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"CNN, accuracy: \" + str(metrics.accuracy_score(y_test, y_pred_cnn)) + \" F1 score:\" + str(metrics.f1_score(y_test, y_pred_cnn, average='weighted')))\n",
        "matrix_cnn = confusion_matrix(y_test, y_pred_cnn)\n",
        "print(matrix_cnn)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxAkXLPQ9oB8",
        "outputId": "3a6bc05b-1edd-400e-e82f-1ef3ccd3b103"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "232/232 - 9s - loss: 6.9204 - accuracy: 0.7693 - val_loss: 2.7288 - val_accuracy: 0.8104 - 9s/epoch - 39ms/step\n",
            "Epoch 2/200\n",
            "232/232 - 1s - loss: 1.6503 - accuracy: 0.8612 - val_loss: 2.2658 - val_accuracy: 0.8487 - 1s/epoch - 4ms/step\n",
            "Epoch 3/200\n",
            "232/232 - 1s - loss: 1.2311 - accuracy: 0.8821 - val_loss: 2.0652 - val_accuracy: 0.8541 - 996ms/epoch - 4ms/step\n",
            "Epoch 4/200\n",
            "232/232 - 1s - loss: 1.3121 - accuracy: 0.8914 - val_loss: 1.7354 - val_accuracy: 0.8784 - 1s/epoch - 5ms/step\n",
            "Epoch 5/200\n",
            "232/232 - 1s - loss: 0.7808 - accuracy: 0.9051 - val_loss: 1.7923 - val_accuracy: 0.8051 - 1s/epoch - 5ms/step\n",
            "Epoch 6/200\n",
            "232/232 - 1s - loss: 0.6118 - accuracy: 0.9085 - val_loss: 1.5959 - val_accuracy: 0.8690 - 997ms/epoch - 4ms/step\n",
            "Epoch 7/200\n",
            "232/232 - 1s - loss: 0.6621 - accuracy: 0.9118 - val_loss: 1.2639 - val_accuracy: 0.8907 - 981ms/epoch - 4ms/step\n",
            "Epoch 8/200\n",
            "232/232 - 1s - loss: 1.4230 - accuracy: 0.8915 - val_loss: 1.9212 - val_accuracy: 0.8358 - 1s/epoch - 5ms/step\n",
            "Epoch 9/200\n",
            "232/232 - 1s - loss: 0.5238 - accuracy: 0.9121 - val_loss: 1.4824 - val_accuracy: 0.8100 - 1s/epoch - 6ms/step\n",
            "Epoch 10/200\n",
            "232/232 - 1s - loss: 0.4375 - accuracy: 0.9186 - val_loss: 1.9595 - val_accuracy: 0.7870 - 1s/epoch - 6ms/step\n",
            "Epoch 11/200\n",
            "232/232 - 1s - loss: 0.6921 - accuracy: 0.9092 - val_loss: 1.7527 - val_accuracy: 0.8247 - 1s/epoch - 6ms/step\n",
            "Epoch 12/200\n",
            "232/232 - 1s - loss: 0.5043 - accuracy: 0.9157 - val_loss: 1.6850 - val_accuracy: 0.7920 - 1s/epoch - 6ms/step\n",
            "Epoch 12: early stopping\n",
            "Training time: 937.3769738674164\n",
            "3103/3103 [==============================] - 8s 3ms/step\n",
            "Test time: 30.303951740264893\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_2 (Conv1D)           (None, 31, 32)            128       \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1  (None, 15, 32)            0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 480)               0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 128)               61568     \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 6)                 774       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 62470 (244.02 KB)\n",
            "Trainable params: 62470 (244.02 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "CNN, accuracy: 0.7919730083593514 F1 score:0.8107425225544687\n",
            "[[ 2894   487     6   210   583   171]\n",
            " [ 3571 31534     1  3867    34    70]\n",
            " [    1     3    89    90     0     1]\n",
            " [    0  3841     0 40474  5262    62]\n",
            " [ 1290   540    16   228  1091   113]\n",
            " [    2     8     0     0   198  2553]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Assuming you have loaded and preprocessed your data into x_train, y_train, x_test, y_test\n",
        "\n",
        "# Create a Sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Add a 1D convolutional layer with 64 filters and kernel size 3\n",
        "model.add(Conv1D(64, kernel_size=3, input_shape=(x_train.shape[1], 1), activation='relu'))\n",
        "\n",
        "# Add a 1D MaxPooling layer with pool size 2\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "# Add another convolutional layer with 128 filters and kernel size 3\n",
        "model.add(Conv1D(128, kernel_size=3, activation='relu'))\n",
        "\n",
        "# Add another MaxPooling layer with pool size 2\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "# Flatten the output of the convolutional layer\n",
        "model.add(Flatten())\n",
        "\n",
        "# Add a fully connected layer with 256 units and ReLU activation\n",
        "model.add(Dense(256, activation='relu'))\n",
        "\n",
        "# Add Dropout for regularization\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Add the output layer with the number of classes and softmax activation\n",
        "model.add(Dense(6, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Define an early stopping callback\n",
        "# monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train, validation_data=(x_test, y_test), callbacks=[monitor], verbose=2, epochs=100, batch_size=1000)\n",
        "\n",
        "# Measure training time\n",
        "end = time.time()\n",
        "diff = end - start\n",
        "print(\"Training time: \" + str(diff))\n",
        "\n",
        "# Make predictions on the test data\n",
        "starttest = time.time()\n",
        "y_pred_cnn = model.predict(x_test)\n",
        "y_pred_cnn = np.argmax(y_pred_cnn, axis=1)\n",
        "endtest = time.time()\n",
        "difftest = endtest - starttest\n",
        "print(\"Test time: \" + str(difftest))\n",
        "\n",
        "# Print model summary\n",
        "print(model.summary())\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"CNN, accuracy: \" + str(metrics.accuracy_score(y_test, y_pred_cnn)) + \" F1 score:\" + str(metrics.f1_score(y_test, y_pred_cnn, average='weighted')))\n",
        "matrix_cnn = confusion_matrix(y_test, y_pred_cnn)\n",
        "print(matrix_cnn)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGIvkbGxAzLD",
        "outputId": "6c1ccf5e-0dea-460a-8f4d-bcf55dbbc91d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "232/232 - 4s - loss: 4.6628 - accuracy: 0.7602 - val_loss: 0.4837 - val_accuracy: 0.8360 - 4s/epoch - 17ms/step\n",
            "Epoch 2/100\n",
            "232/232 - 2s - loss: 0.4909 - accuracy: 0.8467 - val_loss: 0.3759 - val_accuracy: 0.8733 - 2s/epoch - 8ms/step\n",
            "Epoch 3/100\n",
            "232/232 - 1s - loss: 0.3729 - accuracy: 0.8880 - val_loss: 0.3399 - val_accuracy: 0.8823 - 1s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "232/232 - 2s - loss: 0.3284 - accuracy: 0.8996 - val_loss: 0.3493 - val_accuracy: 0.8823 - 2s/epoch - 9ms/step\n",
            "Epoch 5/100\n",
            "232/232 - 2s - loss: 0.3057 - accuracy: 0.9047 - val_loss: 0.3374 - val_accuracy: 0.8893 - 2s/epoch - 9ms/step\n",
            "Epoch 6/100\n",
            "232/232 - 2s - loss: 0.2871 - accuracy: 0.9090 - val_loss: 0.3531 - val_accuracy: 0.8785 - 2s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "232/232 - 2s - loss: 0.2741 - accuracy: 0.9112 - val_loss: 0.3543 - val_accuracy: 0.8569 - 2s/epoch - 9ms/step\n",
            "Epoch 8/100\n",
            "232/232 - 1s - loss: 0.2743 - accuracy: 0.9115 - val_loss: 0.3477 - val_accuracy: 0.8576 - 1s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "232/232 - 1s - loss: 0.2643 - accuracy: 0.9134 - val_loss: 0.3956 - val_accuracy: 0.8558 - 1s/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "232/232 - 2s - loss: 0.2571 - accuracy: 0.9151 - val_loss: 0.3726 - val_accuracy: 0.8558 - 2s/epoch - 8ms/step\n",
            "Epoch 10: early stopping\n",
            "Training time: 1268.1228036880493\n",
            "3103/3103 [==============================] - 13s 4ms/step\n",
            "Test time: 22.005849599838257\n",
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_11 (Conv1D)          (None, 31, 64)            256       \n",
            "                                                                 \n",
            " max_pooling1d_9 (MaxPoolin  (None, 15, 64)            0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_12 (Conv1D)          (None, 13, 128)           24704     \n",
            "                                                                 \n",
            " max_pooling1d_10 (MaxPooli  (None, 6, 128)            0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 768)               0         \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 256)               196864    \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 6)                 1542      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 223366 (872.52 KB)\n",
            "Trainable params: 223366 (872.52 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "CNN, accuracy: 0.8558162956994662 F1 score:0.8547711959558734\n",
            "[[ 3437   646     3   245    20     0]\n",
            " [ 3610 31515     0  3949     3     0]\n",
            " [    2    30    62    90     0     0]\n",
            " [   49  2707     0 46645   175    63]\n",
            " [ 1675   463     5   579   554     2]\n",
            " [    0     0     0     0     0  2761]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Assuming you have loaded and preprocessed your data into x_train, y_train, x_test, y_test\n",
        "\n",
        "# Create a Sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Add a 1D convolutional layer with 64 filters and kernel size 3\n",
        "model.add(Conv1D(64, kernel_size=3, input_shape=(x_train.shape[1], 1), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "# Add another convolutional layer with 128 filters and kernel size 3\n",
        "model.add(Conv1D(128, kernel_size=3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "# Add another convolutional layer with 256 filters and kernel size 3\n",
        "model.add(Conv1D(256, kernel_size=3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "# Flatten the output of the convolutional layer\n",
        "model.add(Flatten())\n",
        "\n",
        "# Add a fully connected layer with 512 units and ReLU activation\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Add another fully connected layer with 256 units and ReLU activation\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Add the output layer with the number of classes and softmax activation\n",
        "model.add(Dense(6, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Define an early stopping callback\n",
        "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train, validation_data=(x_test, y_test), callbacks=[monitor], verbose=2, epochs=200, batch_size=1000)\n",
        "\n",
        "# Measure training time\n",
        "end = time.time()\n",
        "diff = end - start\n",
        "print(\"Training time: \" + str(diff))\n",
        "\n",
        "# Make predictions on the test data\n",
        "starttest = time.time()\n",
        "y_pred_cnn = model.predict(x_test)\n",
        "y_pred_cnn = np.argmax(y_pred_cnn, axis=1)\n",
        "endtest = time.time()\n",
        "difftest = endtest - starttest\n",
        "print(\"Test time: \" + str(difftest))\n",
        "\n",
        "# Print model summary\n",
        "print(model.summary())\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"CNN, accuracy: \" + str(metrics.accuracy_score(y_test, y_pred_cnn)) + \" F1 score:\" + str(metrics.f1_score(y_test, y_pred_cnn, average='weighted')))\n",
        "matrix_cnn = confusion_matrix(y_test, y_pred_cnn)\n",
        "print(matrix_cnn)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dji1Ht5SCQYt",
        "outputId": "56a57a9e-595a-494e-e2cc-e14b3ceb2768"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "232/232 - 6s - loss: 0.5503 - accuracy: 0.7895 - val_loss: 0.6503 - val_accuracy: 0.7844 - 6s/epoch - 27ms/step\n",
            "Epoch 2/200\n",
            "232/232 - 2s - loss: 0.4566 - accuracy: 0.8205 - val_loss: 0.6514 - val_accuracy: 0.7701 - 2s/epoch - 11ms/step\n",
            "Epoch 3/200\n",
            "232/232 - 2s - loss: 0.3683 - accuracy: 0.8585 - val_loss: 2.0179 - val_accuracy: 0.4078 - 2s/epoch - 9ms/step\n",
            "Epoch 4/200\n",
            "232/232 - 2s - loss: 0.2741 - accuracy: 0.9022 - val_loss: 2.5272 - val_accuracy: 0.3944 - 2s/epoch - 9ms/step\n",
            "Epoch 5/200\n",
            "232/232 - 2s - loss: 0.2635 - accuracy: 0.9056 - val_loss: 2.4899 - val_accuracy: 0.5261 - 2s/epoch - 9ms/step\n",
            "Epoch 6/200\n",
            "232/232 - 2s - loss: 0.2493 - accuracy: 0.9108 - val_loss: 3.1107 - val_accuracy: 0.5336 - 2s/epoch - 9ms/step\n",
            "Epoch 6: early stopping\n",
            "Training time: 1353.0507462024689\n",
            "3103/3103 [==============================] - 11s 3ms/step\n",
            "Test time: 20.685484409332275\n",
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_13 (Conv1D)          (None, 31, 64)            256       \n",
            "                                                                 \n",
            " batch_normalization_6 (Bat  (None, 31, 64)            256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling1d_11 (MaxPooli  (None, 15, 64)            0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " conv1d_14 (Conv1D)          (None, 13, 128)           24704     \n",
            "                                                                 \n",
            " batch_normalization_7 (Bat  (None, 13, 128)           512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling1d_12 (MaxPooli  (None, 6, 128)            0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " conv1d_15 (Conv1D)          (None, 4, 256)            98560     \n",
            "                                                                 \n",
            " batch_normalization_8 (Bat  (None, 4, 256)            1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling1d_13 (MaxPooli  (None, 2, 256)            0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " flatten_5 (Flatten)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 512)               262656    \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 256)               131328    \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 6)                 1542      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 520838 (1.99 MB)\n",
            "Trainable params: 519942 (1.98 MB)\n",
            "Non-trainable params: 896 (3.50 KB)\n",
            "_________________________________________________________________\n",
            "None\n",
            "CNN, accuracy: 0.5335683351797764 F1 score:0.4676872511854268\n",
            "[[    0  4334     3    10     4     0]\n",
            " [    0 39064     3     0    10     0]\n",
            " [    1   129    47     0     7     0]\n",
            " [    0 35791     0 13598   250     0]\n",
            " [    0  2781     8   220   269     0]\n",
            " [    0  2761     0     0     0     0]]\n"
          ]
        }
      ]
    }
  ]
}